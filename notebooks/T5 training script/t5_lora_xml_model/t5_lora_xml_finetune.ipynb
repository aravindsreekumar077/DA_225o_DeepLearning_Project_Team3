{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Install required libraries\n",
    "!pip install -U transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Imports\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load tokenizer and base model\n",
    "model_name = 'google/flan-t5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49042037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Prepare model for LoRA\n",
    "model = prepare_model_for_int8_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q', 'v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2798b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load your dataset (adjust path if needed)\n",
    "dataset = load_dataset('json', data_files='calc_dataset.jsonl', split='train')\n",
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Preprocess function\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example['prompt'], max_length=MAX_LENGTH, truncation=True, padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example['output'], max_length=MAX_LENGTH, truncation=True, padding='max_length')\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
    "print('Tokenization complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Split dataset\n",
    "splits = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "train_dataset = splits['train']\n",
    "eval_dataset = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./t5_lora_xml_model',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcec264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Train!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
